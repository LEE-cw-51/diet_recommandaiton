"""
ì„¸ë¸ì¼ë ˆë¸ í¬ë¡¤ëŸ¬ (ìµœì¢… ë²„ì „)
"""
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import pandas as pd
import time
import re
import os
import sys

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from config.settings import settings


class SevenElevenCrawler:
    """ì„¸ë¸ì¼ë ˆë¸ í¬ë¡¤ëŸ¬"""
    
    def __init__(self, headless=False):
        print("ğŸ”§ Chrome ì„¤ì • ì¤‘ (ì„¸ë¸ì¼ë ˆë¸)...")
        
        chrome_options = Options()
        if headless:
            chrome_options.add_argument('--headless')
            print("   (ë¸Œë¼ìš°ì € ìˆ¨ê¹€ ëª¨ë“œ)")
        else:
            print("   (ë¸Œë¼ìš°ì € í‘œì‹œ ëª¨ë“œ)")
        
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
        
        service = Service(ChromeDriverManager().install())
        self.driver = webdriver.Chrome(service=service, options=chrome_options)
        self.base_url = "https://www.7-eleven.co.kr"
        
        print("âœ… ë¸Œë¼ìš°ì € ì¤€ë¹„ ì™„ë£Œ\n")
    
    def get_categories(self):
        """í¬ë¡¤ë§í•  ì¹´í…Œê³ ë¦¬ íƒ­ (URL íŒŒë¼ë¯¸í„°)"""
        return {
            'ì „ì²´': '?',
            'ë„ì‹œë½/ì¡°ë¦¬ë©´': '?pTab=mini',
            'ì‚¼ê°ê¹€ë°¥/ê¹€ë°¥': '?pTab=noodle',
            'ìƒŒë“œìœ„ì¹˜/í–„ë²„ê±°': '?pTab=d_group',
        }
    
    def crawl_category(self, category_name, url_param, max_clicks=20):
        """
        ì¹´í…Œê³ ë¦¬ë³„ í¬ë¡¤ë§
        
        Args:
            category_name: ì¹´í…Œê³ ë¦¬ëª…
            url_param: URL íŒŒë¼ë¯¸í„° (ì˜ˆ: ?pTab=mini)
            max_clicks: ìµœëŒ€ ë”ë³´ê¸° í´ë¦­ íšŸìˆ˜
        """
        print(f"{'='*70}")
        print(f"ğŸ” í¬ë¡¤ë§: ì„¸ë¸ì¼ë ˆë¸ - {category_name}")
        print(f"{'='*70}")
        
        # âœ… URL ìˆ˜ì •
        url = f"{self.base_url}/product/bestdosirakList.asp{url_param}"
        print(f"URL: {url}\n")
        
        try:
            self.driver.get(url)
            time.sleep(3)
            
            all_products = []
            click_count = 0
            no_new_items = 0
            
            while click_count < max_clicks:
                # HTML íŒŒì‹±
                soup = BeautifulSoup(self.driver.page_source, 'html.parser')
                
                # ìƒí’ˆ ëª©ë¡ - í™•ì¸ëœ ì„ íƒì
                items = soup.select('.dosirak_list ul li')
                
                # btn_more ì œì™¸
                items = [item for item in items if 'btn_more' not in item.get('class', [])]
                
                if not items:
                    print("  âš ï¸ ìƒí’ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    break
                
                before_count = len(all_products)
                
                # ê° ìƒí’ˆ íŒŒì‹±
                for item in items:
                    try:
                        # ìƒí’ˆëª…
                        name_elem = item.select_one('.infowrap .name')
                        if not name_elem:
                            continue
                        name = name_elem.text.strip()
                        
                        # ê°€ê²©
                        price_elem = item.select_one('.infowrap .price span')
                        if not price_elem:
                            continue
                        price_text = price_elem.text.strip()
                        price = int(re.sub(r'[^0-9]', '', price_text))
                        
                        # ì´ë¯¸ì§€
                        img_elem = item.select_one('.pic_product img')
                        image_url = ''
                        if img_elem and img_elem.get('src'):
                            image_url = img_elem['src']
                            if not image_url.startswith('http'):
                                image_url = self.base_url + image_url
                        
                        # ìƒí’ˆ ë°ì´í„°
                        product = {
                            'brand_name': 'ì„¸ë¸ì¼ë ˆë¸',
                            'item_name': name,
                            'category': category_name,
                            'price': price,
                            'image_url': image_url,
                        }
                        
                        # ì¤‘ë³µ ì²´í¬
                        if not any(p['item_name'] == name for p in all_products):
                            all_products.append(product)
                    
                    except:
                        continue
                
                # ìƒˆë¡œ ì¶”ê°€ëœ ìƒí’ˆ ìˆ˜
                new_items = len(all_products) - before_count
                
                if new_items == 0:
                    no_new_items += 1
                    if no_new_items >= 2:
                        print(f"  âœ… ë” ì´ìƒ ìƒˆë¡œìš´ ìƒí’ˆì´ ì—†ìŠµë‹ˆë‹¤.")
                        break
                else:
                    no_new_items = 0
                
                print(f"  ğŸ“¦ í´ë¦­ {click_count + 1}: +{new_items}ê°œ (ì´ {len(all_products)}ê°œ)")
                
                # ë”ë³´ê¸° ë²„íŠ¼ ì°¾ê¸° ë° í´ë¦­
                try:
                    more_btn = None
                    selectors = [
                        '.btn_more a',
                        '#moreImg a',
                        'a[href*="fncMore"]',
                    ]
                    
                    for selector in selectors:
                        try:
                            more_btn = self.driver.find_element(By.CSS_SELECTOR, selector)
                            if more_btn.is_displayed():
                                break
                        except:
                            continue
                    
                    if not more_btn:
                        print(f"  âœ… ë”ë³´ê¸° ë²„íŠ¼ ì—†ìŒ")
                        break
                    
                    # ë²„íŠ¼ ìœ„ì¹˜ë¡œ ìŠ¤í¬ë¡¤
                    self.driver.execute_script(
                        "arguments[0].scrollIntoView({block: 'center'});",
                        more_btn
                    )
                    time.sleep(1)
                    
                    # í´ë¦­
                    self.driver.execute_script("arguments[0].click();", more_btn)
                    print(f"     ğŸ–±ï¸  ë”ë³´ê¸° í´ë¦­")
                    
                    # ë¡œë”© ëŒ€ê¸°
                    time.sleep(2)
                    click_count += 1
                    
                except:
                    print(f"  âœ… ë”ë³´ê¸° ì¢…ë£Œ")
                    break
            
            print(f"\nâœ… {category_name} ì™„ë£Œ: {len(all_products)}ê°œ\n")
            return all_products
        
        except Exception as e:
            print(f"\nâŒ {category_name} ì˜¤ë¥˜: {e}\n")
            import traceback
            traceback.print_exc()
            return []
    
    def crawl_all(self, skip_all=True):
        """
        ì „ì²´ ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§
        
        Args:
            skip_all: Trueë©´ 'ì „ì²´' íƒ­ ê±´ë„ˆë›°ê¸° (ì¤‘ë³µ ë°©ì§€)
        """
        categories = self.get_categories()
        all_products = []
        
        print("=" * 70)
        print("ğŸª ì„¸ë¸ì¼ë ˆë¸ ì „ì²´ í¬ë¡¤ë§ ì‹œì‘")
        print("=" * 70)
        
        for cat_name, url_param in categories.items():
            # 'ì „ì²´' íƒ­ì€ ê±´ë„ˆë›°ê¸° (ë‹¤ë¥¸ íƒ­ í•©ì¹˜ë©´ ì¤‘ë³µ)
            if skip_all and cat_name == 'ì „ì²´':
                print(f"â­ï¸  '{cat_name}' íƒ­ ê±´ë„ˆë›°ê¸° (ì¤‘ë³µ ë°©ì§€)\n")
                continue
            
            products = self.crawl_category(cat_name, url_param)
            all_products.extend(products)
            time.sleep(2)
        
        # DataFrame ë³€í™˜
        df = pd.DataFrame(all_products)
        
        if df.empty:
            print("\nâŒ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return df
        
        # ì¤‘ë³µ ì œê±°
        df = df.drop_duplicates(subset=['item_name'], keep='first')
        
        print("\n" + "=" * 70)
        print(f"ğŸ“Š ì„¸ë¸ì¼ë ˆë¸ ì „ì²´ ìˆ˜ì§‘ ì™„ë£Œ")
        print("=" * 70)
        print(f"ì´ ìƒí’ˆ ìˆ˜: {len(df)}ê°œ")
        print(f"\nì¹´í…Œê³ ë¦¬ë³„:")
        print(df['category'].value_counts())
        print(f"\nê°€ê²© í†µê³„:")
        print(df['price'].describe())
        
        return df
    
    def save_to_csv(self, df, filename='seven_products.csv'):
        """CSV ì €ì¥"""
        if df.empty:
            print("âš ï¸ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        filepath = os.path.join(settings.DATA_RAW, filename)
        os.makedirs(settings.DATA_RAW, exist_ok=True)
        
        df.to_csv(filepath, index=False, encoding='utf-8-sig')
        
        print(f"\nğŸ’¾ ì €ì¥ ì™„ë£Œ: {filepath}")
        print(f"\n=== ìƒ˜í”Œ ë°ì´í„° (ì²˜ìŒ 10ê°œ) ===")
        print(df[['item_name', 'category', 'price']].head(10).to_string(index=False))
    
    def close(self):
        """ë¸Œë¼ìš°ì € ì¢…ë£Œ"""
        try:
            self.driver.quit()
            print("\nğŸ”’ ë¸Œë¼ìš°ì € ì¢…ë£Œ")
        except:
            pass


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    print("=" * 70)
    print("ğŸš€ ì„¸ë¸ì¼ë ˆë¸ í¬ë¡¤ëŸ¬ ì‹œì‘")
    print("=" * 70)
    
    crawler = SevenElevenCrawler(headless=False)
    
    try:
        # í…ŒìŠ¤íŠ¸: ë„ì‹œë½/ì¡°ë¦¬ë©´ íƒ­
        print("\nğŸ“ í…ŒìŠ¤íŠ¸ ëª¨ë“œ: ë„ì‹œë½/ì¡°ë¦¬ë©´\n")
        products = crawler.crawl_category('ë„ì‹œë½/ì¡°ë¦¬ë©´', '?pTab=mini', max_clicks=5)
        
        if products:
            print(f"\nâœ… í…ŒìŠ¤íŠ¸ ì„±ê³µ! {len(products)}ê°œ ì œí’ˆ")
            
            # ìƒ˜í”Œ ë°ì´í„°
            print("\n=== ìˆ˜ì§‘ëœ ë°ì´í„° ìƒ˜í”Œ ===")
            for i, p in enumerate(products[:5], 1):
                print(f"{i}. {p['item_name']} - {p['price']:,}ì›")
            
            # ì „ì²´ í¬ë¡¤ë§ ì—¬ë¶€
            print("\n" + "=" * 70)
            user_input = input("ì „ì²´ ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ì„ ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ")
            
            if user_input.lower() == 'y':
                print("\nì „ì²´ í¬ë¡¤ë§ ì‹œì‘...\n")
                df = crawler.crawl_all(skip_all=True)
                
                if not df.empty:
                    crawler.save_to_csv(df)
            else:
                print("\ní¬ë¡¤ë§ ì¢…ë£Œ")
        else:
            print("\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
    
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ì‚¬ìš©ìê°€ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
    
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        crawler.close()
        print("\nâœ… í”„ë¡œê·¸ë¨ ì¢…ë£Œ")


if __name__ == "__main__":
    main()