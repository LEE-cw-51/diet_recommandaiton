"""
ì´ë§ˆíŠ¸24 í¬ë¡¤ëŸ¬ (ìµœì¢… ë²„ì „)
"""
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import pandas as pd
import time
import re
import os
import sys

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from config.settings import settings


class Emart24Crawler:
    """ì´ë§ˆíŠ¸24 í¬ë¡¤ëŸ¬"""
    
    def __init__(self, headless=False):
        print("ğŸ”§ Chrome ì„¤ì • ì¤‘ (ì´ë§ˆíŠ¸24)...")
        
        chrome_options = Options()
        if headless:
            chrome_options.add_argument('--headless')
            print("   (ë¸Œë¼ìš°ì € ìˆ¨ê¹€ ëª¨ë“œ)")
        else:
            print("   (ë¸Œë¼ìš°ì € í‘œì‹œ ëª¨ë“œ)")
        
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
        
        service = Service(ChromeDriverManager().install())
        self.driver = webdriver.Chrome(service=service, options=chrome_options)
        self.base_url = "https://emart24.co.kr"
        
        print("âœ… ë¸Œë¼ìš°ì € ì¤€ë¹„ ì™„ë£Œ\n")
    
    def get_categories(self):
        """í¬ë¡¤ë§í•  ì¹´í…Œê³ ë¦¬ (base_category_seq íŒŒë¼ë¯¸í„°)"""
        return {
            'ì „ì²´': '',
            'ë„ì‹œë½': '41',
            'ê¹€ë°¥': '42',
            'í–„ë²„ê±°': '43',
            'ì£¼ë¨¹ë°¥': '45',
            'ìƒŒë“œìœ„ì¹˜': '46',
            'ì¦‰ì„ì‹': '47',
        }
    
    def crawl_category(self, category_name, base_category_seq, max_pages=10):
        """
        ì¹´í…Œê³ ë¦¬ë³„ í¬ë¡¤ë§ (í˜ì´ì§€ë„¤ì´ì…˜)
        
        Args:
            category_name: ì¹´í…Œê³ ë¦¬ëª…
            base_category_seq: ì¹´í…Œê³ ë¦¬ ID (41=ë„ì‹œë½, 42=ê¹€ë°¥ ë“±)
            max_pages: ìµœëŒ€ í˜ì´ì§€ ìˆ˜
        """
        print(f"{'='*70}")
        print(f"ğŸ” í¬ë¡¤ë§: ì´ë§ˆíŠ¸24 - {category_name}")
        print(f"{'='*70}")
        
        all_products = []
        
        for page in range(1, max_pages + 1):
            # URL ìƒì„±
            if base_category_seq:
                url = f"{self.base_url}/goods/ff?search=&category_seq=&base_category_seq={base_category_seq}&align=&page={page}"
            else:
                url = f"{self.base_url}/goods/ff?search=&category_seq=&align=&page={page}"
            
            print(f"  ğŸ“„ í˜ì´ì§€ {page}: {url}")
            
            try:
                self.driver.get(url)
                time.sleep(3)
                
                # HTML íŒŒì‹±
                soup = BeautifulSoup(self.driver.page_source, 'html.parser')
                
                # ìƒí’ˆ ëª©ë¡
                items = soup.select('.itemList .itemWrap')
                
                if not items:
                    print(f"  âœ… í˜ì´ì§€ {page}ì— ìƒí’ˆ ì—†ìŒ - í¬ë¡¤ë§ ì¢…ë£Œ")
                    break
                
                before_count = len(all_products)
                
                # ê° ìƒí’ˆ íŒŒì‹±
                for item in items:
                    try:
                        # ìƒí’ˆëª…
                        name_elem = item.select_one('.itemtitle p a')
                        if not name_elem:
                            continue
                        name = name_elem.text.strip()
                        
                        # ê°€ê²©
                        price_elem = item.select_one('.price')
                        if not price_elem:
                            continue
                        price_text = price_elem.text.strip()
                        # "2,400 ì›" â†’ 2400
                        price = int(re.sub(r'[^0-9]', '', price_text))
                        
                        # ì´ë¯¸ì§€
                        img_elem = item.select_one('.itemSpImg img')
                        image_url = ''
                        if img_elem and img_elem.get('src'):
                            image_url = img_elem['src']
                        
                        # ìƒí’ˆ ë°ì´í„°
                        product = {
                            'brand_name': 'ì´ë§ˆíŠ¸24',
                            'item_name': name,
                            'category': category_name,
                            'price': price,
                            'image_url': image_url,
                        }
                        
                        # ì¤‘ë³µ ì²´í¬
                        if not any(p['item_name'] == name for p in all_products):
                            all_products.append(product)
                    
                    except Exception as e:
                        continue
                
                new_items = len(all_products) - before_count
                print(f"     +{new_items}ê°œ (ì´ {len(all_products)}ê°œ)")
                
                # ìƒí’ˆì´ ì—†ìœ¼ë©´ ì¢…ë£Œ
                if new_items == 0:
                    print(f"  âœ… ë” ì´ìƒ ìƒí’ˆ ì—†ìŒ")
                    break
                
                time.sleep(2)  # í˜ì´ì§€ ê°„ ëŒ€ê¸°
                
            except Exception as e:
                print(f"  âŒ í˜ì´ì§€ {page} ì˜¤ë¥˜: {e}")
                break
        
        print(f"\nâœ… {category_name} ì™„ë£Œ: {len(all_products)}ê°œ\n")
        return all_products
    
    def crawl_all(self, skip_all=True):
        """
        ì „ì²´ ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§
        
        Args:
            skip_all: Trueë©´ 'ì „ì²´' ì¹´í…Œê³ ë¦¬ ê±´ë„ˆë›°ê¸° (ì¤‘ë³µ ë°©ì§€)
        """
        categories = self.get_categories()
        all_products = []
        
        print("=" * 70)
        print("ğŸª ì´ë§ˆíŠ¸24 ì „ì²´ í¬ë¡¤ë§ ì‹œì‘")
        print("=" * 70)
        
        for cat_name, base_seq in categories.items():
            # 'ì „ì²´' ì¹´í…Œê³ ë¦¬ ê±´ë„ˆë›°ê¸°
            if skip_all and cat_name == 'ì „ì²´':
                print(f"â­ï¸  '{cat_name}' ì¹´í…Œê³ ë¦¬ ê±´ë„ˆë›°ê¸° (ì¤‘ë³µ ë°©ì§€)\n")
                continue
            
            products = self.crawl_category(cat_name, base_seq, max_pages=10)
            all_products.extend(products)
            time.sleep(2)
        
        # DataFrame ë³€í™˜
        df = pd.DataFrame(all_products)
        
        if df.empty:
            print("\nâŒ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return df
        
        # ì¤‘ë³µ ì œê±°
        df = df.drop_duplicates(subset=['item_name'], keep='first')
        
        print("\n" + "=" * 70)
        print(f"ğŸ“Š ì´ë§ˆíŠ¸24 ì „ì²´ ìˆ˜ì§‘ ì™„ë£Œ")
        print("=" * 70)
        print(f"ì´ ìƒí’ˆ ìˆ˜: {len(df)}ê°œ")
        print(f"\nì¹´í…Œê³ ë¦¬ë³„:")
        print(df['category'].value_counts())
        print(f"\nê°€ê²© í†µê³„:")
        print(df['price'].describe())
        
        return df
    
    def save_to_csv(self, df, filename='emart24_products.csv'):
        """CSV ì €ì¥"""
        if df.empty:
            print("âš ï¸ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        filepath = os.path.join(settings.DATA_RAW, filename)
        os.makedirs(settings.DATA_RAW, exist_ok=True)
        
        df.to_csv(filepath, index=False, encoding='utf-8-sig')
        
        print(f"\nğŸ’¾ ì €ì¥ ì™„ë£Œ: {filepath}")
        print(f"\n=== ìƒ˜í”Œ ë°ì´í„° (ì²˜ìŒ 10ê°œ) ===")
        print(df[['item_name', 'category', 'price']].head(10).to_string(index=False))
    
    def close(self):
        """ë¸Œë¼ìš°ì € ì¢…ë£Œ"""
        try:
            self.driver.quit()
            print("\nğŸ”’ ë¸Œë¼ìš°ì € ì¢…ë£Œ")
        except:
            pass


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    print("=" * 70)
    print("ğŸš€ ì´ë§ˆíŠ¸24 í¬ë¡¤ëŸ¬ ì‹œì‘")
    print("=" * 70)
    
    crawler = Emart24Crawler(headless=False)
    
    try:
        # í…ŒìŠ¤íŠ¸: ë„ì‹œë½ ì¹´í…Œê³ ë¦¬
        print("\nğŸ“ í…ŒìŠ¤íŠ¸ ëª¨ë“œ: ë„ì‹œë½ ì¹´í…Œê³ ë¦¬\n")
        products = crawler.crawl_category('ë„ì‹œë½', '41', max_pages=3)
        
        if products:
            print(f"\nâœ… í…ŒìŠ¤íŠ¸ ì„±ê³µ! {len(products)}ê°œ ì œí’ˆ")
            
            # ìƒ˜í”Œ ë°ì´í„°
            print("\n=== ìˆ˜ì§‘ëœ ë°ì´í„° ìƒ˜í”Œ ===")
            for i, p in enumerate(products[:5], 1):
                print(f"{i}. {p['item_name']} - {p['price']:,}ì›")
            
            # ì „ì²´ í¬ë¡¤ë§ ì—¬ë¶€
            print("\n" + "=" * 70)
            user_input = input("ì „ì²´ ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ì„ ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ")
            
            if user_input.lower() == 'y':
                print("\nì „ì²´ í¬ë¡¤ë§ ì‹œì‘...\n")
                df = crawler.crawl_all(skip_all=True)
                
                if not df.empty:
                    crawler.save_to_csv(df)
            else:
                print("\ní¬ë¡¤ë§ ì¢…ë£Œ")
        else:
            print("\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
    
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ì‚¬ìš©ìê°€ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
    
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        crawler.close()
        print("\nâœ… í”„ë¡œê·¸ë¨ ì¢…ë£Œ")


if __name__ == "__main__":
    main()