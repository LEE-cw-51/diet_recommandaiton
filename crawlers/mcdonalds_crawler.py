"""
ë§¥ë„ë‚ ë“œ í†µí•© í¬ë¡¤ëŸ¬ (ì˜ì–‘ì„±ë¶„ + ì•Œë ˆë¥´ê¸° ì •ë³´)
* ì˜ì–‘ì •ë³´ í˜ì´ì§€ì™€ ì•Œë ˆë¥´ê¸° ì •ë³´ í˜ì´ì§€ë¥¼ ëª¨ë‘ í¬ë¡¤ë§í•˜ì—¬ ë°ì´í„°ë¥¼ ë³‘í•©í•©ë‹ˆë‹¤.
* ìƒì„±ëœ ë°ì´í„°ëŠ” DBì˜ Menu_Master í…Œì´ë¸” êµ¬ì¡°ì™€ í˜¸í™˜ë©ë‹ˆë‹¤.
"""
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import pandas as pd
import time
import re
import os
import sys

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì„¤ì •
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    from config.settings import settings
except ImportError:
    class MockSettings:
        DATA_RAW = './data/raw'
    settings = MockSettings()

class McDonaldsCrawler:
    def __init__(self, headless=True):
        print("ğŸ”§ Chrome ì„¤ì • ì¤‘ (ë§¥ë„ë‚ ë“œ í†µí•© í¬ë¡¤ëŸ¬)...")
        chrome_options = Options()
        if headless:
            chrome_options.add_argument('--headless')
        
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('user-agent=Mozilla/5.0')
        
        service = Service(ChromeDriverManager().install())
        self.driver = webdriver.Chrome(service=service, options=chrome_options)
        self.wait = WebDriverWait(self.driver, 10)
        self.base_url = "https://www.mcdonalds.co.kr"

    def crawl_nutrition_table(self):
        """ì˜ì–‘ì •ë³´ í˜ì´ì§€ í¬ë¡¤ë§"""
        url = f"{self.base_url}/kor/menu/information/nutrition"
        print(f"\n1ï¸âƒ£ ì˜ì–‘ì •ë³´ ìˆ˜ì§‘ ì‹œì‘: {url}")
        self.driver.get(url)
        time.sleep(2) # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
        
        products = {} # Key: ë©”ë‰´ëª…, Value: ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        
        try:
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            # ëª¨ë“  ì¹´í…Œê³ ë¦¬ í…Œì´ë¸” ìˆœíšŒ
            tables = soup.select('table')
            
            for table in tables:
                # í—¤ë” ë§¤í•‘ í™•ì¸
                headers = [th.text.strip() for th in table.select('thead th')]
                col_map = {}
                for idx, h in enumerate(headers):
                    if 'ì—´ëŸ‰' in h: col_map[idx] = 'calories'
                    elif 'ë‹¹' in h: col_map[idx] = 'sugars'
                    elif 'ë‹¨ë°±ì§ˆ' in h: col_map[idx] = 'protein'
                    elif 'í¬í™”ì§€ë°©' in h: col_map[idx] = 'saturated_fat'
                    elif 'ë‚˜íŠ¸ë¥¨' in h: col_map[idx] = 'sodium'
                
                # í–‰ ë°ì´í„° ì¶”ì¶œ
                for row in table.select('tbody tr'):
                    cols = row.select('th, td')
                    if not cols: continue
                    
                    name = cols[0].text.strip()
                    product_data = {
                        'menu_name': name,
                        'store_name': 'McDonalds',
                        'price': 0, # ê°€ê²© ì •ë³´ ì—†ìŒ
                        # 9ëŒ€ ì˜ì–‘ì†Œ ì´ˆê¸°í™”
                        'calories': 0.0, 'carbs': 0.0, 'sugars': 0.0, 'protein': 0.0, 'fat': 0.0,
                        'saturated_fat': 0.0, 'trans_fat': 0.0, 'cholesterol': 0.0, 'sodium': 0.0,
                        'ingredients_raw': '',
                        'allergens_scraped': ''
                    }
                    
                    # ë§¤í•‘ëœ ì˜ì–‘ì†Œ ê°’ ì¶”ì¶œ
                    for idx, col in enumerate(cols):
                        if idx in col_map:
                            text = col.text.strip()
                            # ìˆ«ìë§Œ ì¶”ì¶œ (ë²”ìœ„ ê°’ì¸ ê²½ìš° ìµœì†Œê°’ ì‚¬ìš©)
                            nums = re.findall(r'[\d.]+', text)
                            if nums:
                                product_data[col_map[idx]] = float(nums[0])
                    
                    products[name] = product_data
                    
            print(f"   ğŸ‘‰ {len(products)}ê°œ ë©”ë‰´ ì˜ì–‘ì •ë³´ í™•ë³´")
            return products
            
        except Exception as e:
            print(f"âŒ ì˜ì–‘ì •ë³´ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return {}

    def crawl_allergy_table(self):
        """ì•Œë ˆë¥´ê¸° ì •ë³´ í˜ì´ì§€ í¬ë¡¤ë§"""
        url = f"{self.base_url}/kor/menu/information/allergens"
        print(f"\n2ï¸âƒ£ ì•Œë ˆë¥´ê¸° ì •ë³´ ìˆ˜ì§‘ ì‹œì‘: {url}")
        self.driver.get(url)
        time.sleep(2)
        
        allergy_map = {} # Key: ë©”ë‰´ëª…, Value: ì•Œë ˆë¥´ê¸° ì •ë³´ ë¬¸ìì—´
        
        try:
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            tables = soup.select('table')
            
            for table in tables:
                # ì•Œë ˆë¥´ê¸° í…Œì´ë¸”ì€ ë³´í†µ [ë©”ë‰´ëª…, ì•Œë ˆë¥´ê¸° ìœ ë°œ ì‹ì¬ë£Œ] êµ¬ì¡°ì„
                for row in table.select('tbody tr'):
                    cols = row.select('th, td')
                    if len(cols) < 2: continue
                    
                    name = cols[0].text.strip()
                    allergens = cols[1].text.strip()
                    
                    allergy_map[name] = allergens
            
            print(f"   ğŸ‘‰ {len(allergy_map)}ê°œ ë©”ë‰´ ì•Œë ˆë¥´ê¸° ì •ë³´ í™•ë³´")
            return allergy_map
            
        except Exception as e:
            print(f"âŒ ì•Œë ˆë¥´ê¸° ì •ë³´ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return {}

    def run(self):
        # 1. ì˜ì–‘ì •ë³´ ìˆ˜ì§‘
        products = self.crawl_nutrition_table()
        
        # 2. ì•Œë ˆë¥´ê¸° ì •ë³´ ìˆ˜ì§‘
        allergens = self.crawl_allergy_table()
        
        # 3. ë°ì´í„° ë³‘í•© (ë©”ë‰´ëª… ê¸°ì¤€)
        print("\n3ï¸âƒ£ ë°ì´í„° ë³‘í•© ì¤‘...")
        merged_list = []
        
        for name, data in products.items():
            # ì•Œë ˆë¥´ê¸° ì •ë³´ê°€ ìˆìœ¼ë©´ ì¶”ê°€
            if name in allergens:
                data['allergens_scraped'] = allergens[name]
            
            # ì¹´í…Œê³ ë¦¬ ì¶”ì • (ë‹¨ìˆœ ë¡œì§)
            if 'ë²„ê±°' in name: data['category'] = 'ë²„ê±°'
            elif 'ì„¸íŠ¸' in name: data['category'] = 'ì„¸íŠ¸'
            elif 'ë¨¸í•€' in name: data['category'] = 'ë§¥ëª¨ë‹'
            elif 'ì•„ë©”ë¦¬ì¹´ë…¸' in name or 'ë¼ë–¼' in name or 'ì‰ì´í¬' in name: data['category'] = 'ìŒë£Œ'
            else: data['category'] = 'ì‚¬ì´ë“œ/ë””ì €íŠ¸'
            
            merged_list.append(data)
            
        # 4. CSV ì €ì¥
        if merged_list:
            df = pd.DataFrame(merged_list)
            
            # DB ì»¬ëŸ¼ ìˆœì„œëŒ€ë¡œ ì •ë ¬
            columns = [
                'store_name', 'menu_name', 'price', 'calories', 'carbs', 'sugars', 
                'protein', 'fat', 'saturated_fat', 'trans_fat', 'cholesterol', 
                'sodium', 'ingredients_raw', 'allergens_scraped', 'category'
            ]
            
            # ì—†ëŠ” ì»¬ëŸ¼ì€ 0ì´ë‚˜ ë¹ˆ ê°’ìœ¼ë¡œ ì±„ì›€
            for col in columns:
                if col not in df.columns:
                    df[col] = 0 if 'fat' in col or 'chol' in col else ''
            
            os.makedirs(settings.DATA_RAW, exist_ok=True)
            filepath = os.path.join(settings.DATA_RAW, 'mcdonalds_products.csv')
            df[columns].to_csv(filepath, index=False, encoding='utf-8-sig')
            print(f"\nğŸ’¾ ì €ì¥ ì™„ë£Œ: {filepath} (ì´ {len(df)}ê°œ ë©”ë‰´)")
        else:
            print("\nâš ï¸ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    def close(self):
        self.driver.quit()

if __name__ == "__main__":
    crawler = McDonaldsCrawler(headless=True)
    crawler.run()
    crawler.close()