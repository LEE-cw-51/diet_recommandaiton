"""
GS25 í¸ì˜ì  í¬ë¡¤ëŸ¬ (í˜ì´ì§€ë„¤ì´ì…˜ í´ë¦­ ë°©ì‹)
"""
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import NoSuchElementException, TimeoutException
from bs4 import BeautifulSoup
import pandas as pd
import time
import re
import os
import sys

# settings.py íŒŒì¼ì´ ìƒìœ„ í´ë”ì— ìˆëŠ” ê²½ìš°
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
# 'config' í´ë”ê°€ í˜„ì¬ í´ë”ì™€ ê°™ì€ ìœ„ì¹˜ì— ìˆë‹¤ë©´ ìœ„ ë¼ì¸ì€ ì£¼ì„ ì²˜ë¦¬í•˜ê³  ì•„ë˜ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
# from config.settings import settings
# --- settings ëª¨ë“ˆ ì„ì‹œ ì„¤ì • (í…ŒìŠ¤íŠ¸ìš©) ---
# ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ì´ ë¶€ë¶„ì„ ì§€ìš°ê³  ìœ„ì˜ importë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
class MockSettings:
    DATA_RAW = './data_raw'
settings = MockSettings()
# --- ì„ì‹œ ì„¤ì • ë ---


class GS25Crawler:
    """GS25 í¸ì˜ì  í¬ë¡¤ëŸ¬"""
    
    def __init__(self, headless=False):
        print("ğŸ”§ Chrome ì„¤ì • ì¤‘ (GS25)...")
        
        chrome_options = Options()
        if headless:
            chrome_options.add_argument('--headless')
            print("   (ë¸Œë¼ìš°ì € ìˆ¨ê¹€ ëª¨ë“œ)")
        else:
            print("   (ë¸Œë¼ìš°ì € í‘œì‹œ ëª¨ë“œ)")
        
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
        
        try:
            service = Service(ChromeDriverManager().install())
            self.driver = webdriver.Chrome(service=service, options=chrome_options)
        except ValueError as e:
            print(f"âŒ ChromeDriverManager ì˜¤ë¥˜: {e}")
            print("   Chrome ë“œë¼ì´ë²„ ìˆ˜ë™ ì„¤ì¹˜ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            raise
            
        self.wait = WebDriverWait(self.driver, 10)
        
        print("âœ… ë¸Œë¼ìš°ì € ì¤€ë¹„ ì™„ë£Œ\n")
    
    def get_categories(self):
        """í¬ë¡¤ë§í•  ì¹´í…Œê³ ë¦¬ íƒ­ ID"""
        return {
            'ì „ì²´': 'productALL',
            'ë„ì‹œë½': 'productLunch',
            'ê¹€ë°¥/ì£¼ë¨¹ë°¥': 'productRice',
            'í–„ë²„ê±°/ìƒŒë“œìœ„ì¹˜': 'productBurger',
            'ê°„í¸ì‹': 'productSnack',
        }
    
    def crawl_category(self, category_name, tab_id):
        """
        ì¹´í…Œê³ ë¦¬ë³„ í¬ë¡¤ë§ (í˜ì´ì§€ë„¤ì´ì…˜ í´ë¦­ ë°©ì‹)
        
        Args:
            category_name: ì¹´í…Œê³ ë¦¬ëª…
            tab_id: íƒ­ ë²„íŠ¼ ID
        """
        print(f"{'='*70}")
        print(f"ğŸ” í¬ë¡¤ë§: GS25 - {category_name}")
        print(f"{'='*70}")
        
        try:
            # íƒ­ í´ë¦­
            try:
                tab_button = self.wait.until(
                    EC.element_to_be_clickable((By.ID, tab_id))
                )
                self.driver.execute_script("arguments[0].click();", tab_button)
                print(f"   ğŸ–±ï¸  '{category_name}' íƒ­ í´ë¦­")
                time.sleep(3)  # ì½˜í…ì¸  ë¡œë”© ëŒ€ê¸°
            except Exception as e:
                print(f"   âŒ íƒ­ í´ë¦­ ì‹¤íŒ¨: {e}")
                return []
            
            all_products = []
            page_count = 1
            no_new_items_streak = 0
            
            # --- [ìˆ˜ì •ë¨] ìŠ¤í¬ë¡¤ ë£¨í”„ ëŒ€ì‹  í˜ì´ì§€ë„¤ì´ì…˜ ë£¨í”„ ---
            while True:
                print(f"   ğŸ“„ í˜ì´ì§€ {page_count} íŒŒì‹± ì¤‘...")
                
                try:
                    # ìƒí’ˆ ëª©ë¡ì´ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°
                    self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "ul.prod_list > li")))
                    time.sleep(1) # JSê°€ DOMì„ ì™„ì „íˆ ê·¸ë¦´ ë•Œê¹Œì§€ ì ì‹œ ëŒ€ê¸°
                except TimeoutException:
                    print("   âš ï¸ ìƒí’ˆ ëª©ë¡ì„ ê¸°ë‹¤ë ¸ì§€ë§Œ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                    if page_count == 1:
                        print("   âš ï¸ ì´ ì¹´í…Œê³ ë¦¬ì— ìƒí’ˆì´ ì—†ëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤.")
                    break # ë£¨í”„ ì¢…ë£Œ

                soup = BeautifulSoup(self.driver.page_source, 'html.parser')
                
                # HTML êµ¬ì¡°ì— ë§ì¶° ì…€ë ‰í„° êµ¬ì²´í™”
                items = soup.select('ul.prod_list > li .prod_box')
                
                if not items and page_count == 1:
                    print("   âš ï¸ ìƒí’ˆ íƒœê·¸(.prod_box)ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    break

                before_count = len(all_products)
                
                # ê° ìƒí’ˆ íŒŒì‹± (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼)
                for item in items:
                    try:
                        name_elem = item.select_one('.tit')
                        if not name_elem: continue
                        name = name_elem.text.strip()
                        
                        price_elem = item.select_one('.cost')
                        if not price_elem: continue
                        price_text = price_elem.text.strip()
                        price = int(re.sub(r'[^0-9]', '', price_text))
                        
                        img_elem = item.select_one('.img img')
                        image_url = ''
                        if img_elem and img_elem.get('src'):
                            image_url = img_elem['src']
                        
                        product = {
                            'brand_name': 'GS25',
                            'item_name': name,
                            'category': category_name,
                            'price': price,
                            'image_url': image_url,
                        }
                        
                        # ì¤‘ë³µ ì²´í¬
                        if not any(p['item_name'] == name for p in all_products):
                            all_products.append(product)
                    
                    except Exception as e:
                        print(f"    - ìƒí’ˆ íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
                        continue
                
                new_items_found = len(all_products) - before_count
                print(f"   ğŸ“¦ +{new_items_found}ê°œ ì‹ ê·œ ìƒí’ˆ (ì´ {len(all_products)}ê°œ)")
                
                # --- [ìˆ˜ì •ë¨] í˜ì´ì§€ë„¤ì´ì…˜ ë¡œì§ ---
                
                # 2í˜ì´ì§€ ì—°ì†ìœ¼ë¡œ ìƒˆ ìƒí’ˆì´ ì—†ìœ¼ë©´ ì¢…ë£Œ (ì¤‘ë³µ í˜ì´ì§€ ë°©ì§€)
                if new_items_found == 0 and page_count > 1:
                    no_new_items_streak += 1
                    if no_new_items_streak >= 2:
                        print("   âœ… ìƒˆë¡œìš´ ìƒí’ˆì´ ì—†ì–´ í¬ë¡¤ë§ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                        break
                else:
                    no_new_items_streak = 0
                
                try:
                    # 'ë‹¤ìŒ' ë²„íŠ¼(>)ì„ ì°¾ìŠµë‹ˆë‹¤. (HTML: <a class="next" ...>)
                    next_button = self.driver.find_element(By.CSS_SELECTOR, "a.next[onclick*='moveControl']")
                    
                    # ë‹¤ìŒ ë²„íŠ¼ í´ë¦­
                    self.driver.execute_script("arguments[0].click();", next_button)
                    print(f"   â–¶ï¸ ë‹¤ìŒ í˜ì´ì§€({page_count + 1})ë¡œ ì´ë™...")
                    page_count += 1
                    time.sleep(3) # ìƒˆ í˜ì´ì§€ AJAX ë¡œë“œ ëŒ€ê¸°

                except NoSuchElementException:
                    # 'ë‹¤ìŒ' ë²„íŠ¼ì´ ë” ì´ìƒ ì—†ìœ¼ë©´ ë§ˆì§€ë§‰ í˜ì´ì§€ì…ë‹ˆë‹¤.
                    print(f"   âœ… ë‹¤ìŒ í˜ì´ì§€ ë²„íŠ¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. '{category_name}' ì™„ë£Œ.")
                    break
                except Exception as e:
                    print(f"   âŒ ë‹¤ìŒ í˜ì´ì§€ í´ë¦­ ì¤‘ ì˜¤ë¥˜: {e}")
                    break
            
            print(f"\nâœ… {category_name} ì™„ë£Œ: {len(all_products)}ê°œ\n")
            return all_products
        
        except Exception as e:
            print(f"\nâŒ {category_name} ì˜¤ë¥˜: {e}\n")
            import traceback
            traceback.print_exc()
            return []
    
    def crawl_all(self, skip_all=True):
        """
        ì „ì²´ ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§
        
        Args:
            skip_all: Trueë©´ 'ì „ì²´' íƒ­ ê±´ë„ˆë›°ê¸° (ì¤‘ë³µ ë°©ì§€)
        """
        main_url = "http://gs25.gsretail.com/gscvs/ko/products/youus-freshfood"
        
        print("=" * 70)
        print("ğŸª GS25 ì „ì²´ í¬ë¡¤ë§ ì‹œì‘")
        print("=" * 70)
        
        try:
            self.driver.get(main_url)
            print(f"ğŸ“¡ í˜ì´ì§€ ì ‘ì†: {main_url}\n")
            time.sleep(3)
            
            categories = self.get_categories()
            all_products = []
            
            for cat_name, tab_id in categories.items():
                if skip_all and cat_name == 'ì „ì²´':
                    print(f"â­ï¸  '{cat_name}' íƒ­ ê±´ë„ˆë›°ê¸° (ì¤‘ë³µ ë°©ì§€)\n")
                    continue
                
                # [ìˆ˜ì •ë¨] max_scrolls ì¸ìˆ˜ ì œê±°
                products = self.crawl_category(cat_name, tab_id)
                all_products.extend(products)
                time.sleep(2) # íƒ­ ì´ë™ ê°„ ê°„ê²©
            
            if not all_products:
                 print("\nâŒ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                 return pd.DataFrame()
                 
            df = pd.DataFrame(all_products)
            
            # ì¤‘ë³µ ì œê±°
            df = df.drop_duplicates(subset=['item_name'], keep='first')
            
            print("\n" + "=" * 70)
            print(f"ğŸ“Š GS25 ì „ì²´ ìˆ˜ì§‘ ì™„ë£Œ")
            print("=" * 70)
            print(f"ì´ ìƒí’ˆ ìˆ˜: {len(df)}ê°œ")
            print(f"\nì¹´í…Œê³ ë¦¬ë³„:")
            print(df['category'].value_counts())
            print(f"\nê°€ê²© í†µê³„:")
            print(df['price'].describe())
            
            return df
        
        except Exception as e:
            print(f"\nâŒ ì „ì²´ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            return pd.DataFrame()
    
    def save_to_csv(self, df, filename='gs25_products.csv'):
        """CSV ì €ì¥"""
        if df.empty:
            print("âš ï¸ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        os.makedirs(settings.DATA_RAW, exist_ok=True)
        filepath = os.path.join(settings.DATA_RAW, filename)
        
        df.to_csv(filepath, index=False, encoding='utf-8-sig')
        
        print(f"\nğŸ’¾ ì €ì¥ ì™„ë£Œ: {filepath}")
        print(f"\n=== ìƒ˜í”Œ ë°ì´í„° (ì²˜ìŒ 10ê°œ) ===")
        print(df[['item_name', 'category', 'price']].head(10).to_string(index=False))
    
    def close(self):
        """ë¸Œë¼ìš°ì € ì¢…ë£Œ"""
        try:
            self.driver.quit()
            print("\nğŸ”’ ë¸Œë¼ìš°ì € ì¢…ë£Œ")
        except:
            pass


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    print("=" * 70)
    print("ğŸš€ GS25 í¬ë¡¤ëŸ¬ ì‹œì‘")
    print("=" * 70)
    
    # headless=Trueë¡œ ë³€ê²½í•˜ë©´ ë¸Œë¼ìš°ì € ì°½ì´ ëœ¨ì§€ ì•ŠìŠµë‹ˆë‹¤.
    crawler = GS25Crawler(headless=False) 
    
    try:
        main_url = "http://gs25.gsretail.com/gscvs/ko/products/youus-freshfood"
        print(f"ğŸ“¡ í˜ì´ì§€ ì ‘ì†: {main_url}\n")
        
        crawler.driver.get(main_url)
        time.sleep(3)
        
        # í…ŒìŠ¤íŠ¸: ë„ì‹œë½ íƒ­ë§Œ
        print("ğŸ“ í…ŒìŠ¤íŠ¸ ëª¨ë“œ: ë„ì‹œë½ íƒ­\n")
        
        # [ìˆ˜ì •ë¨] max_scrolls ì¸ìˆ˜ ì œê±°
        products = crawler.crawl_category('ë„ì‹œë½', 'productLunch')
        
        if products:
            print(f"\nâœ… í…ŒìŠ¤íŠ¸ ì„±ê³µ! {len(products)}ê°œ ì œí’ˆ")
            
            print("\n=== ìˆ˜ì§‘ëœ ë°ì´í„° ìƒ˜í”Œ ===")
            for i, p in enumerate(products[:5], 1):
                print(f"{i}. {p['item_name']} - {p['price']:,}ì›")
            
            print("\n" + "=" * 70)
            user_input = input("ì „ì²´ ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ì„ ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ")
            
            if user_input.lower() == 'y':
                print("\nì „ì²´ í¬ë¡¤ë§ ì‹œì‘...\n")
                
                # crawl_allì´ ë‚´ë¶€ì ìœ¼ë¡œ í˜ì´ì§€ë¥¼ ìƒˆë¡œ ë¡œë“œí•˜ë¯€ë¡œ
                # ì—¬ê¸°ì„œëŠ” ë³„ë„ë¡œ get()ì„ í˜¸ì¶œí•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤.
                df = crawler.crawl_all(skip_all=True)
                
                if not df.empty:
                    crawler.save_to_csv(df, 'gs25_fresh_food.csv')
            else:
                print("\ní¬ë¡¤ë§ ì¢…ë£Œ")
        else:
            print("\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
    
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ì‚¬ìš©ìê°€ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
    
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        crawler.close()
        print("\nâœ… í”„ë¡œê·¸ë¨ ì¢…ë£Œ")


if __name__ == "__main__":
    main()