"""
CU í¸ì˜ì  í¬ë¡¤ëŸ¬ (ìµœì¢… ë²„ì „)
"""
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import pandas as pd
import time
import re
import os
import sys

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from config.settings import settings


class CUCrawler:
    """CU í¸ì˜ì  í¬ë¡¤ëŸ¬"""
    
    def __init__(self, headless=False):
        """
        ì´ˆê¸°í™”
        
        Args:
            headless: Trueë©´ ë¸Œë¼ìš°ì € ìˆ¨ê¹€
        """
        print("ğŸ”§ Chrome ì„¤ì • ì¤‘...")
        
        chrome_options = Options()
        if headless:
            chrome_options.add_argument('--headless')
            print("   (ë¸Œë¼ìš°ì € ìˆ¨ê¹€ ëª¨ë“œ)")
        else:
            print("   (ë¸Œë¼ìš°ì € í‘œì‹œ ëª¨ë“œ)")
        
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-blink-features=AutomationControlled')
        chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
        
        try:
            service = Service(ChromeDriverManager().install())
            self.driver = webdriver.Chrome(service=service, options=chrome_options)
            print("âœ… ë¸Œë¼ìš°ì € ì¤€ë¹„ ì™„ë£Œ\n")
        except Exception as e:
            print(f"âŒ ë¸Œë¼ìš°ì € ì„¤ì • ì‹¤íŒ¨: {e}")
            raise
        
        self.base_url = "https://cu.bgfretail.com"
    
    def get_category_name_from_url(self, url):
        """URLì—ì„œ ì¹´í…Œê³ ë¦¬ ì´ë¦„ ì¶”ì •"""
        if 'depth2=4' in url:
            if 'depth3=1' in url:
                return 'ë„ì‹œë½'
            elif 'depth3=2' in url:
                return 'ìƒŒë“œìœ„ì¹˜'
            elif 'depth3=3' in url:
                return 'í–„ë²„ê±°'
            elif 'depth3=4' in url:
                return 'ì£¼ë¨¹ë°¥'
            elif 'depth3=5' in url:
                return 'ê¹€ë°¥'
            else:
                return 'ê°„í¸ì‹ì‚¬'
        return 'ê¸°íƒ€'
    
    def crawl_page(self, url, category_name=None, max_clicks=20):
        """
        í˜ì´ì§€ í¬ë¡¤ë§
        
        Args:
            url: í¬ë¡¤ë§í•  URL
            category_name: ì¹´í…Œê³ ë¦¬ëª… (Noneì´ë©´ URLì—ì„œ ì¶”ì •)
            max_clicks: ìµœëŒ€ ë”ë³´ê¸° í´ë¦­ íšŸìˆ˜
        
        Returns:
            list: ìƒí’ˆ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        if category_name is None:
            category_name = self.get_category_name_from_url(url)
        
        print(f"{'='*70}")
        print(f"ğŸ” í¬ë¡¤ë§: {category_name}")
        print(f"{'='*70}")
        print(f"URL: {url}\n")
        
        try:
            # í˜ì´ì§€ ì ‘ì†
            self.driver.get(url)
            time.sleep(3)
            
            all_products = []
            click_count = 0
            no_new_items = 0
            
            while click_count < max_clicks:
                # HTML íŒŒì‹±
                soup = BeautifulSoup(self.driver.page_source, 'html.parser')
                
                # ìƒí’ˆ ëª©ë¡ - í™•ì¸ëœ ì„ íƒì ì‚¬ìš©
                items = soup.select('.prod_item')
                
                if not items:
                    print("âŒ ìƒí’ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    # ë””ë²„ê¹…ìš© HTML ì¶œë ¥
                    print("\n=== HTML ìƒ˜í”Œ ===")
                    print(soup.prettify()[:1500])
                    break
                
                before_count = len(all_products)
                
                # ê° ìƒí’ˆ íŒŒì‹±
                for item in items:
                    try:
                        # ìƒí’ˆëª…
                        name_elem = item.select_one('.prod_text .name p')
                        if not name_elem:
                            continue
                        name = name_elem.text.strip()
                        
                        # ê°€ê²©
                        price_elem = item.select_one('.prod_text .price strong')
                        if not price_elem:
                            continue
                        price_text = price_elem.text.strip()
                        # ì‰¼í‘œ ì œê±° í›„ ìˆ«ìë§Œ
                        price = int(re.sub(r'[^0-9]', '', price_text))
                        
                        # ì´ë¯¸ì§€
                        img_elem = item.select_one('.prod_img img')
                        image_url = ''
                        if img_elem and img_elem.get('src'):
                            image_url = img_elem['src']
                            # //ë¡œ ì‹œì‘í•˜ë©´ https: ì¶”ê°€
                            if image_url.startswith('//'):
                                image_url = 'https:' + image_url
                            elif not image_url.startswith('http'):
                                image_url = self.base_url + image_url
                        
                        # ìƒí’ˆ ë°ì´í„°
                        product = {
                            'brand_name': 'CU',
                            'item_name': name,
                            'category': category_name,
                            'price': price,
                            'image_url': image_url,
                        }
                        
                        # ì¤‘ë³µ ì²´í¬
                        if not any(p['item_name'] == name for p in all_products):
                            all_products.append(product)
                    
                    except Exception as e:
                        # ê°œë³„ ìƒí’ˆ íŒŒì‹± ì‹¤íŒ¨ëŠ” ì¡°ìš©íˆ ë„˜ì–´ê°
                        continue
                
                # ìƒˆë¡œ ì¶”ê°€ëœ ìƒí’ˆ ìˆ˜
                new_items = len(all_products) - before_count
                
                if new_items == 0:
                    no_new_items += 1
                    if no_new_items >= 2:
                        print(f"  âœ… ë” ì´ìƒ ìƒˆë¡œìš´ ìƒí’ˆì´ ì—†ìŠµë‹ˆë‹¤.")
                        break
                else:
                    no_new_items = 0
                
                print(f"  ğŸ“¦ ì‹œë„ {click_count + 1}: +{new_items}ê°œ (ì´ {len(all_products)}ê°œ)")
                
                # ë”ë³´ê¸° ë²„íŠ¼ ì°¾ê¸° ë° í´ë¦­
                try:
                    more_btn = None
                    
                    # ì—¬ëŸ¬ ì„ íƒì ì‹œë„
                    btn_selectors = [
                        'a.btn_more',
                        'button.btn_more',
                        '.prodListBtn a',
                        'a[onclick*="more"]',
                    ]
                    
                    for selector in btn_selectors:
                        try:
                            more_btn = self.driver.find_element(By.CSS_SELECTOR, selector)
                            if more_btn.is_displayed():
                                break
                        except:
                            continue
                    
                    if not more_btn:
                        print(f"  âœ… ë”ë³´ê¸° ë²„íŠ¼ ì—†ìŒ - ëª¨ë“  ìƒí’ˆ ë¡œë“œ ì™„ë£Œ")
                        break
                    
                    # ë²„íŠ¼ ìœ„ì¹˜ë¡œ ìŠ¤í¬ë¡¤
                    self.driver.execute_script(
                        "arguments[0].scrollIntoView({block: 'center'});", 
                        more_btn
                    )
                    time.sleep(1)
                    
                    # í´ë¦­
                    self.driver.execute_script("arguments[0].click();", more_btn)
                    print(f"     ğŸ–±ï¸  ë”ë³´ê¸° í´ë¦­")
                    
                    # ë¡œë”© ëŒ€ê¸°
                    time.sleep(2)
                    click_count += 1
                    
                except Exception as e:
                    print(f"  âœ… ë”ë³´ê¸° ì¢…ë£Œ")
                    break
            
            print(f"\nâœ… {category_name} ì™„ë£Œ: {len(all_products)}ê°œ ìƒí’ˆ\n")
            return all_products
        
        except Exception as e:
            print(f"\nâŒ {category_name} í¬ë¡¤ë§ ì˜¤ë¥˜: {e}\n")
            import traceback
            traceback.print_exc()
            return []
    
    def crawl_all_categories(self):
        """ì „ì²´ ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§"""
        categories = {
            'ë„ì‹œë½': f'{self.base_url}/product/product.do?category=product&depth2=4&depth3=1&sf=N',
            'ìƒŒë“œìœ„ì¹˜': f'{self.base_url}/product/product.do?category=product&depth2=4&depth3=2&sf=N',
            'í–„ë²„ê±°': f'{self.base_url}/product/product.do?category=product&depth2=4&depth3=3&sf=N',
            'ì£¼ë¨¹ë°¥': f'{self.base_url}/product/product.do?category=product&depth2=4&depth3=4&sf=N',
            'ê¹€ë°¥': f'{self.base_url}/product/product.do?category=product&depth2=4&depth3=5&sf=N',
        }
        
        all_products = []
        
        print("=" * 70)
        print("ğŸª CU í¸ì˜ì  ì „ì²´ í¬ë¡¤ë§ ì‹œì‘")
        print("=" * 70)
        
        for cat_name, cat_url in categories.items():
            products = self.crawl_page(cat_url, cat_name)
            all_products.extend(products)
            time.sleep(2)  # ì¹´í…Œê³ ë¦¬ ê°„ ëŒ€ê¸°
        
        # DataFrame ë³€í™˜
        df = pd.DataFrame(all_products)
        
        if df.empty:
            print("\nâŒ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return df
        
        # ì¤‘ë³µ ì œê±°
        df = df.drop_duplicates(subset=['item_name'], keep='first')
        
        print("\n" + "=" * 70)
        print(f"ğŸ“Š ì „ì²´ ìˆ˜ì§‘ ì™„ë£Œ")
        print("=" * 70)
        print(f"ì´ ìƒí’ˆ ìˆ˜: {len(df)}ê°œ")
        print(f"\nì¹´í…Œê³ ë¦¬ë³„:")
        print(df['category'].value_counts())
        print(f"\nê°€ê²© í†µê³„:")
        print(df['price'].describe())
        
        return df
    
    def save_to_csv(self, df, filename='cu_products.csv'):
        """
        CSV ì €ì¥
        
        Args:
            df: ì €ì¥í•  DataFrame
            filename: íŒŒì¼ëª…
        """
        if df.empty:
            print("âš ï¸ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # data/raw í´ë”ì— ì €ì¥
        filepath = os.path.join(settings.DATA_RAW, filename)
        os.makedirs(settings.DATA_RAW, exist_ok=True)
        
        df.to_csv(filepath, index=False, encoding='utf-8-sig')
        
        print(f"\nğŸ’¾ ì €ì¥ ì™„ë£Œ: {filepath}")
        print(f"\n=== ìƒ˜í”Œ ë°ì´í„° (ì²˜ìŒ 10ê°œ) ===")
        print(df[['item_name', 'category', 'price']].head(10).to_string(index=False))
    
    def close(self):
        """ë¸Œë¼ìš°ì € ì¢…ë£Œ"""
        try:
            self.driver.quit()
            print("\nğŸ”’ ë¸Œë¼ìš°ì € ì¢…ë£Œ")
        except:
            pass


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    print("=" * 70)
    print("ğŸš€ CU í¬ë¡¤ëŸ¬ ì‹œì‘")
    print("=" * 70)
    
    # í¬ë¡¤ëŸ¬ ìƒì„± (ë¸Œë¼ìš°ì € ë³´ë ¤ë©´ headless=False)
    crawler = CUCrawler(headless=False)
    
    try:
        # í…ŒìŠ¤íŠ¸: ë„ì‹œë½ë§Œ
        print("\nğŸ“ í…ŒìŠ¤íŠ¸ ëª¨ë“œ: ë„ì‹œë½ ì¹´í…Œê³ ë¦¬\n")
        test_url = "https://cu.bgfretail.com/product/product.do?category=product&depth2=4&depth3=1&sf=N"
        products = crawler.crawl_page(test_url, max_clicks=3)
        
        if products:
            print(f"\nâœ… í…ŒìŠ¤íŠ¸ ì„±ê³µ! {len(products)}ê°œ ì œí’ˆ ìˆ˜ì§‘")
            
            # ìƒ˜í”Œ ë°ì´í„° í™•ì¸
            print("\n=== ìˆ˜ì§‘ëœ ë°ì´í„° ìƒ˜í”Œ ===")
            for i, p in enumerate(products[:5], 1):
                print(f"{i}. {p['item_name']} - {p['price']:,}ì›")
            
            # ì „ì²´ í¬ë¡¤ë§ ì—¬ë¶€
            print("\n" + "=" * 70)
            user_input = input("ì „ì²´ ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ì„ ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ")
            
            if user_input.lower() == 'y':
                print("\nì „ì²´ í¬ë¡¤ë§ ì‹œì‘...\n")
                df = crawler.crawl_all_categories()
                
                if not df.empty:
                    crawler.save_to_csv(df)
            else:
                print("\ní¬ë¡¤ë§ ì¢…ë£Œ")
        else:
            print("\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: ìƒí’ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ì‚¬ìš©ìê°€ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
    
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        crawler.close()
        print("\nâœ… í”„ë¡œê·¸ë¨ ì¢…ë£Œ")


if __name__ == "__main__":
    main()