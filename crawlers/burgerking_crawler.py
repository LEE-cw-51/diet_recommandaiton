"""
ë²„ê±°í‚¹ ì…”í‹€ ë”œë¦¬ë²„ë¦¬ í¬ë¡¤ëŸ¬ (ê°€ê²© ì¶”ì¶œ ìµœì í™” ë²„ì „)
* ì œê³µëœ HTML êµ¬ì¡°(ì…”í‹€ ë”œë¦¬ë²„ë¦¬)ì— ë§ì¶° ê°€ê²©ì„ í¬í•¨í•˜ì—¬ ë©”ë‰´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
"""
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
from bs4 import BeautifulSoup
import pandas as pd
import time
import re
import os
import sys

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# --- settings ëª¨ë“ˆ ì„ì‹œ ì„¤ì • (ì‹¤ì œ í™˜ê²½ì— ë§ê²Œ ìˆ˜ì • í•„ìš”) ---
class MockSettings:
    DATA_RAW = './data_raw'
settings = MockSettings()
# --- ì„ì‹œ ì„¤ì • ë ---

class BurgerKingShuttleCrawler:
    """ë²„ê±°í‚¹ ì…”í‹€ ë”œë¦¬ë²„ë¦¬ ë©”ë‰´ ë° ê°€ê²© í¬ë¡¤ëŸ¬"""
    
    def __init__(self, headless=False):
        print("ğŸ”§ Chrome ì„¤ì • ì¤‘ (ë²„ê±°í‚¹ ì…”í‹€ ë”œë¦¬ë²„ë¦¬)...")
        
        chrome_options = Options()
        if headless:
            chrome_options.add_argument('--headless')
            print("   (ë¸Œë¼ìš°ì € ìˆ¨ê¹€ ëª¨ë“œ)")
        else:
            print("   (ë¸Œë¼ìš°ì € í‘œì‹œ ëª¨ë“œ)")
        
        # PC ì›¹ í™˜ê²½ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
        
        try:
            service = Service(ChromeDriverManager().install())
            self.driver = webdriver.Chrome(service=service, options=chrome_options)
        except Exception as e:
            print(f"âŒ ChromeDriverManager ì˜¤ë¥˜: {e}")
            raise
            
        self.wait = WebDriverWait(self.driver, 15)
        print("âœ… ë¸Œë¼ìš°ì € ì¤€ë¹„ ì™„ë£Œ\n")
        
    def crawl_all(self):
        """ì „ì²´ ë©”ë‰´ ë° ê°€ê²© í¬ë¡¤ë§"""
        # ì…”í‹€ ë”œë¦¬ë²„ë¦¬ ë²„ê±°í‚¹ ë©”ë‰´ í˜ì´ì§€ URL
        main_url = "https://www.shuttledelivery.co.kr/ko/restaurant/menu/2510"
        
        print("=" * 70)
        print("ğŸ” ë²„ê±°í‚¹ ì…”í‹€ ë”œë¦¬ë²„ë¦¬ ë©”ë‰´ í¬ë¡¤ë§ ì‹œì‘")
        print("=" * 70)
        
        all_products = []
        
        try:
            self.driver.get(main_url)
            print(f"ğŸ“¡ í˜ì´ì§€ ì ‘ì†: {main_url}")
            
            # 1. ë©”ë‰´ ëª©ë¡ ì»¨í…Œì´ë„ˆê°€ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°
            self.wait.until(EC.presence_of_element_located((By.ID, 'leftBasketColumn')))
            time.sleep(3) # ì¶”ê°€ ë¡œë”© ëŒ€ê¸°

            # 2. HTML íŒŒì‹±
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            
            # 3. ì¹´í…Œê³ ë¦¬ë³„ ì„¹ì…˜ ì¶”ì¶œ (ì˜ˆ: ì™€í¼ì„¸íŠ¸, ì™€í¼ì£¼ë‹ˆì–´ ì„¸íŠ¸, ì™€í¼, ì‚¬ì´ë“œ, ìŒë£Œ)
            menu_sections = soup.select('#leftBasketColumn .single-menu')
            
            if not menu_sections:
                print("âŒ ë©”ë‰´ ì„¹ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ì„ íƒì: #leftBasketColumn .single-menu)")
                return pd.DataFrame()

            print(f"ğŸ“¦ ì´ {len(menu_sections)}ê°œ ë©”ë‰´ ì„¹ì…˜ ë°œê²¬. íŒŒì‹± ì‹œì‘...")

            for section in menu_sections:
                # ì¹´í…Œê³ ë¦¬ëª… ì¶”ì¶œ (ì˜ˆ: "ì™€í¼ì„¸íŠ¸")
                category_elem = section.select_one('.headingTitle')
                if not category_elem:
                    continue
                category_name = category_elem.text.strip()
                
                print(f"  > ì¹´í…Œê³ ë¦¬: {category_name}")

                # í•´ë‹¹ ì„¹ì…˜ ë‚´ì˜ ëª¨ë“  ë©”ë‰´ ì•„ì´í…œ ì¶”ì¶œ
                items = section.select('.items .menuitem')
                
                for item in items:
                    try:
                        # ë©”ë‰´ëª…: .itemtitle (ëŒ€ë¶€ë¶„ì˜ ë©”ë‰´ëª…ì€ ì—¬ê¸°ì— ìˆìŒ)
                        name_elem = item.select_one('.itemtitle')
                        name = name_elem.text.strip() if name_elem else "ì´ë¦„ ì—†ìŒ"
                        
                        # ê°€ê²©: .price
                        price_elem = item.select_one('.price')
                        price_text = price_elem.text.strip() if price_elem else None
                        
                        price = None
                        if price_text:
                            # ê°€ê²© í…ìŠ¤íŠ¸ì—ì„œ 'ì›'ê³¼ ì‰¼í‘œë¥¼ ì œê±°í•˜ê³  ì •ìˆ˜ë¡œ ë³€í™˜
                            price = int(re.sub(r'[^\d]', '', price_text))
                        
                        # ì„¤ëª…: p íƒœê·¸ (ë©”ë‰´ ìƒì„¸ ì„¤ëª…)
                        description_elem = item.select_one('.titlecol p')
                        description = description_elem.text.strip() if description_elem else ''

                        # ì´ë¯¸ì§€ URL: menupage-thumbnailì˜ data-original ì†ì„±
                        img_anchor = item.select_one('.menupage-thumbnail')
                        image_url = img_anchor.get('data-original', '') if img_anchor else ''
                        
                        product = {
                            'brand_name': 'ë²„ê±°í‚¹',
                            'item_name': name,
                            'category': category_name,
                            'price': price,
                            'description': description,
                            'image_url': image_url,
                        }
                        
                        # ê°€ê²©ì´ ì—†ëŠ” í•­ëª©ì€ ì œì™¸í•˜ê±°ë‚˜ ê²½ê³ ë¥¼ í‘œì‹œí•  ìˆ˜ ìˆì§€ë§Œ, ì¼ë‹¨ ëª¨ë‘ ìˆ˜ì§‘
                        if price is not None:
                            all_products.append(product)
                        
                    except Exception as e:
                        print(f"    - [WARN] ê°œë³„ ë©”ë‰´ íŒŒì‹± ì˜¤ë¥˜ (ì´ì „ ë©”ë‰´: {name if 'name' in locals() else 'Unknown'}): {e}")
                        continue

            if not all_products:
                 print("\nâŒ ìˆ˜ì§‘ëœ ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                 return pd.DataFrame()
                 
            df = pd.DataFrame(all_products)
            df = df.drop_duplicates(subset=['item_name'], keep='first')
            
            print("\n" + "=" * 70)
            print(f"ğŸ“Š ë²„ê±°í‚¹ ë”œë¦¬ë²„ë¦¬ ë©”ë‰´ ë° ê°€ê²© ìˆ˜ì§‘ ì™„ë£Œ")
            print("=" * 70)
            print(f"ì´ ìƒí’ˆ ìˆ˜: {len(df)}ê°œ")
            print(f"\nì¹´í…Œê³ ë¦¬ë³„:")
            print(df['category'].value_counts())
            
            return df
        
        except TimeoutException:
            print("\nâŒ í˜ì´ì§€ ë¡œë“œ ì‹œê°„ ì´ˆê³¼. (ë„¤íŠ¸ì›Œí¬ ë¬¸ì œ ë˜ëŠ” ë¡œë”© ì§€ì—°)")
            return pd.DataFrame()
        except Exception as e:
            print(f"\nâŒ ì „ì²´ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            return pd.DataFrame()
    
    def save_to_csv(self, df, filename='burgerking_shuttle_delivery_menu.csv'):
        """CSV ì €ì¥"""
        if df.empty:
            print("âš ï¸ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        os.makedirs(settings.DATA_RAW, exist_ok=True)
        filepath = os.path.join(settings.DATA_RAW, filename)
        
        # 'description' ì—´ ì¶”ê°€
        df.to_csv(filepath, index=False, encoding='utf-8-sig')
        
        print(f"\nğŸ’¾ ì €ì¥ ì™„ë£Œ: {filepath}")
        print(f"\n=== ìƒ˜í”Œ ë°ì´í„° (ì²˜ìŒ 10ê°œ) ===")
        print(df[['item_name', 'category', 'price', 'description']].head(10).to_string(index=False))
    
    def close(self):
        """ë¸Œë¼ìš°ì € ì¢…ë£Œ"""
        try:
            self.driver.quit()
            print("\nğŸ”’ ë¸Œë¼ìš°ì € ì¢…ë£Œ")
        except:
            pass


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    # ë”œë¦¬ë²„ë¦¬ ì›¹ì‚¬ì´íŠ¸ëŠ” ëª¨ë°”ì¼ ì—ë®¬ë ˆì´ì…˜ ì—†ì´ë„ ì˜ ì‘ë™í•˜ë¯€ë¡œ ì¼ë°˜ PC ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.
    crawler = BurgerKingShuttleCrawler(headless=False) 
    
    try:
        df = crawler.crawl_all()
        
        if not df.empty:
            crawler.save_to_csv(df)
    
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ì‚¬ìš©ìê°€ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ë©”ì¸ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        crawler.close()
        print("\nâœ… í”„ë¡œê·¸ë¨ ì¢…ë£Œ")


if __name__ == "__main__":
    main()