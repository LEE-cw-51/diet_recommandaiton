"""
ë²„ê±°í‚¹ í†µí•© í¬ë¡¤ëŸ¬ (ìµœì¢… ì™„ì„±ë³¸ - ì˜ì–‘ì„±ë¶„ íŒŒì‹± ë¡œì§ ê°•í™”)
* ëª¨ë‹¬ íŒì—… ë‚´ë¶€ë¥¼ ìŠ¤í¬ë¡¤í•˜ê³ , í…Œì´ë¸”ì„ ì°¾ì•„ 9ê°€ì§€ ì˜ì–‘ì†Œë¥¼ ì •í™•íˆ ì¶”ì¶œí•©ë‹ˆë‹¤.
* ì˜ì–‘ì†Œ í•­ëª©ì´ ëˆ„ë½ëœ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ë”•ì…”ë„ˆë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì•ˆì •ì„±ì„ í™•ë³´í–ˆìŠµë‹ˆë‹¤.
"""
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException, StaleElementReferenceException
from bs4 import BeautifulSoup
import pandas as pd
import time
import re
import os
import sys

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

try:
    from config.settings import settings
except ImportError:
    class MockSettings:
        DATA_RAW = './data_raw'
    settings = MockSettings()

# ì˜ì–‘ì†Œ ì´ë¦„ê³¼ DB ì»¬ëŸ¼ëª… ë§¤í•‘ (ì •ê·œì‹ ì²˜ë¦¬ë¥¼ ìœ„í•´ í‚¤ì›Œë“œë§Œ ì‚¬ìš©)
NUTRITION_KEYWORDS = {
    'ì—´ëŸ‰': 'calories', 'íƒ„ìˆ˜í™”ë¬¼': 'carbs', 'ë‹¹ë¥˜': 'sugars', 'ë‹¨ë°±ì§ˆ': 'protein', 
    'ì§€ë°©': 'fat', 'í¬í™”ì§€ë°©': 'saturated_fat', 'íŠ¸ëœìŠ¤ì§€ë°©': 'trans_fat', 
    'ì½œë ˆìŠ¤í…Œë¡¤': 'cholesterol', 'ë‚˜íŠ¸ë¥¨': 'sodium'
}

class BurgerKingCrawler:
    def __init__(self, headless=True):
        print("ğŸ”§ Chrome ì„¤ì • ì¤‘ (ë²„ê±°í‚¹ - ìµœì¢… íŒŒì‹±)...")
        chrome_options = Options()
        if headless: 
            chrome_options.add_argument('--headless')
        
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--window-size=1920,1080')
        chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
        
        service = Service(ChromeDriverManager().install())
        self.driver = webdriver.Chrome(service=service, options=chrome_options)
        self.wait = WebDriverWait(self.driver, 15) 
        self.base_url = "https://www.burgerking.co.kr"
        print("âœ… ë¸Œë¼ìš°ì € ì¤€ë¹„ ì™„ë£Œ")

    def scrape_nutrition_modal(self, menu_name, category_name):
        """ëª¨ë‹¬ ë‚´ ìŠ¤í¬ë¡¤ ë° ë°ì´í„° ìˆ˜ì§‘ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        product = {
            'store_name': 'BurgerKing',
            'menu_name': menu_name,
            'price': 0, 'category': category_name,
            'calories': 0.0, 'carbs': 0.0, 'sugars': 0.0, 'protein': 0.0, 'fat': 0.0,
            'saturated_fat': 0.0, 'trans_fat': 0.0, 'cholesterol': 0.0, 'sodium': 0.0,
            'ingredients_raw': '', 'allergens_scraped': ''
        }
        
        MODAL_WRAPPER_SELECTOR = ".modalWrap:not([style*='display: none'])"
        MODAL_CONTENT_SELECTOR = f"{MODAL_WRAPPER_SELECTOR} .pop_cont"

        try:
            # 1. ì˜ì–‘ì„±ë¶„ ë²„íŠ¼ í´ë¦­
            btn = self.wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, ".btn_info_link")))
            self.driver.execute_script("arguments[0].click();", btn)
            
            # 2. ëª¨ë‹¬ ì½˜í…ì¸ ê°€ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°
            self.wait.until(EC.visibility_of_element_located((By.CSS_SELECTOR, MODAL_CONTENT_SELECTOR)))
            time.sleep(1)

            # 3. ëª¨ë‹¬ ë‚´ë¶€ë¥¼ ëê¹Œì§€ ìŠ¤í¬ë¡¤í•˜ì—¬ Lazy Loading ë°ì´í„° ë¡œë“œ
            modal_content_element = self.driver.find_element(By.CSS_SELECTOR, MODAL_CONTENT_SELECTOR)
            self.driver.execute_script("arguments[0].scrollTop = arguments[0].scrollHeight", modal_content_element)
            time.sleep(1)
            
            # 4. ë°ì´í„° íŒŒì‹±
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            target_modal = soup.select_one(MODAL_CONTENT_SELECTOR.split(" .pop_cont")[0]) 
            
            if target_modal:
                container = target_modal.select_one('.pop_cont')
                
                # --- ì˜ì–‘ ì„±ë¶„ ì¶”ì¶œ: í…Œì´ë¸” íŒŒì‹± ---
                # ëª¨ë‹¬ ë‚´ ëª¨ë“  í…Œì´ë¸”ì„ ì°¾ìŠµë‹ˆë‹¤. (ê°€ì¥ ë„“ì€ ë²”ìœ„ì˜ íƒìƒ‰)
                tables = container.select('table')
                
                # í…Œì´ë¸”ì´ ì—†ëŠ” ê²½ìš°ë¥¼ ëŒ€ë¹„í•˜ì—¬ ì¼ë°˜ì ì¸ ë¦¬ìŠ¤íŠ¸ êµ¬ì¡° íƒìƒ‰ (.tit02ì™€ ê°’)
                nutrition_items = container.select('.pop_cont .tit02') 
                
                # ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ë§Œë“¤ì–´ ëª¨ë“  ì˜ì–‘ì†Œ ì •ë³´ë¥¼ ë‹´ìŠµë‹ˆë‹¤.
                all_nutrition_items = []

                # 4.1. í…Œì´ë¸” ê¸°ë°˜ ì¶”ì¶œ (ê°€ì¥ í”í•œ í˜•íƒœ)
                for table in tables:
                    rows = table.select('tr')
                    for row in rows:
                        # cols = í–‰ ë‚´ì˜ ëª¨ë“  td/th ìš”ì†Œ
                        cols = row.select('td, th')
                        
                        if len(cols) >= 2 and cols[0].text:
                            all_nutrition_items.append((cols[0].text.strip(), cols[1].text.strip()))
                            
                # 4.2. ë¦¬ìŠ¤íŠ¸ ê¸°ë°˜ ì¶”ì¶œ (í…Œì´ë¸”ì´ ì•„ë‹Œ ê²½ìš°)
                # ì´ ë¡œì§ì€ í…Œì´ë¸”ì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ì‚¬ìš©ë˜ì§€ë§Œ, ì¼ë‹¨ ëª¨ë“  í…ìŠ¤íŠ¸ ìŒì„ ì°¾ìŠµë‹ˆë‹¤.
                # (ì´ ë¶€ë¶„ì€ ë””ë²„ê¹… íŒŒì¼ í™•ì¸ í›„ ê°€ì¥ ì •í™•í•œ ì„ íƒìë¡œ ëŒ€ì²´ ê°€ëŠ¥)

                # ì¶”ì¶œëœ í•­ëª©ì„ ìµœì¢… product ë”•ì…”ë„ˆë¦¬ì— ë§¤í•‘
                for name_raw, val_raw in all_nutrition_items:
                    for keyword, db_col in NUTRITION_KEYWORDS.items():
                        if keyword in name_raw:
                            # ìˆ«ìë§Œ ì¶”ì¶œ (ì˜ˆ: '570 kcal' -> 570)
                            val = float(re.sub(r'[^\d.]', '', val_raw)) if re.search(r'\d', val_raw) else 0.0
                            product[db_col] = val
                            break
                
                # --- ì•Œë ˆë¥´ê¸° ì •ë³´ ì¶”ì¶œ ---
                full_text = container.get_text(separator=' | ', strip=True)
                product['allergens_scraped'] = full_text[:500]

        except Exception as e:
            print(f"   âš ï¸ ëª¨ë‹¬ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

        # 5. ëª¨ë‹¬ ë‹«ê¸°
        finally:
            try:
                # í•˜ë‹¨ 'í™•ì¸' ë²„íŠ¼ í´ë¦­ì„ í†µí•´ ëª¨ë‹¬ ë‹«ê¸°
                close_btn_script = "document.querySelector('.modalWrap:not([style*=\"none\"]) .pop_foot button').click();"
                self.driver.execute_script(close_btn_script)
                self.wait.until(EC.invisibility_of_element_located((By.CSS_SELECTOR, MODAL_WRAPPER_SELECTOR)), timeout=3)
            except:
                pass

        return product

    def run(self):
        self.driver.get("https://www.burgerking.co.kr/menu/main")
        time.sleep(3)
        
        all_products = []
        
        category_selectors = [
            'input[value="cat_K200003"] + .txt_box', 'input[value="cat_K200004"] + .txt_box', 
            'input[value="cat_K200005"] + .txt_box', 'input[value="cat_K200006"] + .txt_box', 
            'input[value="cat_K200010"] + .txt_box', 'input[value="cat_K200020"] + .txt_box',
        ]

        category_names = ['í”„ë¦¬ë¯¸ì—„', 'ì™€í¼&ì£¼ë‹ˆì–´', 'ì¹˜í‚¨&ìŠˆë¦¼í”„ë²„ê±°', 'ì˜¬ë°ì´ìŠ¤ë‚µ&ì˜¬ë°ì´í‚¹', 'ì‚¬ì´ë“œ', 'ìŒë£Œ&ë””ì €íŠ¸']
        
        print(f"ğŸ” í¬ë¡¤ë§ ì‹œì‘: ì´ {len(category_names)}ê°œ ì¹´í…Œê³ ë¦¬")

        # 1. ì „ì²´ ìŠ¤í¬ë¡¤ ë° ë©”ë‰´ ë¡œë”©
        self.scroll_to_bottom()

        # 2. ê·¸ë£¹ë³„ ìˆœíšŒ (íƒ­ í´ë¦­ì€ ì´ì œ í•„ìš” ì—†ìŒ, ì „ì²´ ë¦¬ìŠ¤íŠ¸ì—ì„œ ê·¸ë£¹ë³„ë¡œ ì²˜ë¦¬)
        # ëª¨ë“  ë©”ë‰´ê°€ ë¡œë“œëœ ìƒíƒœì—ì„œ, í˜ì´ì§€ ë‚´ì˜ ëª¨ë“  ë©”ë‰´ ë¦¬ìŠ¤íŠ¸ ì»¨í…Œì´ë„ˆë¥¼ ì°¾ìŠµë‹ˆë‹¤.
        groups = self.driver.find_elements(By.CSS_SELECTOR, ".menu_list_wrap .divide_group")
        group_count = len(groups)
        print(f"ğŸ” ì´ {group_count}ê°œì˜ ë©”ë‰´ ê·¸ë£¹ ë°œê²¬")
        
        for g_idx in range(group_count):
            try:
                # DOM ì¬íƒìƒ‰: ê·¸ë£¹ ìš”ì†Œ ë‹¤ì‹œ ì°¾ê¸°
                groups = self.driver.find_elements(By.CSS_SELECTOR, ".menu_list_wrap .divide_group")
                current_group = groups[g_idx]
                
                try:
                    cat_name = current_group.find_element(By.CSS_SELECTOR, ".tit01").text.strip()
                except:
                    cat_name = "ê¸°íƒ€"
                
                cards = current_group.find_elements(By.CSS_SELECTOR, ".menu_list li .menu_card")
                card_count = len(cards)
                print(f"\nğŸ“‚ [{cat_name}] ì§„ì… - {card_count}ê°œ ë©”ë‰´")

                for i in range(card_count):
                    try:
                        # DOM ì¬íƒìƒ‰: ì¹´ë“œ ìš”ì†Œ ë‹¤ì‹œ ì°¾ê¸°
                        current_cards = current_group.find_elements(By.CSS_SELECTOR, ".menu_list li .menu_card")
                        if i >= len(current_cards): break
                        
                        card = current_cards[i]
                        menu_name = card.find_element(By.CSS_SELECTOR, ".tit").text.strip()
                        
                        print(f"   âœ… ìˆ˜ì§‘ ì¤‘ ({i+1}/{card_count}): {menu_name}", end="\r")
                        
                        # í™”ë©´ ì¤‘ì•™ìœ¼ë¡œ ìŠ¤í¬ë¡¤ (í´ë¦­ ì˜¤ë¥˜ ë°©ì§€)
                        self.driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", card)
                        time.sleep(0.5)

                        # ìƒì„¸ í˜ì´ì§€ ì§„ì… ë° ë°ì´í„° ìˆ˜ì§‘
                        detail_btn = card.find_element(By.CSS_SELECTOR, "button.btn_detail")
                        self.driver.execute_script("arguments[0].click();", detail_btn)
                        
                        self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, ".prd_detailWrap")))
                        time.sleep(1)
                        
                        # ë°ì´í„° ìˆ˜ì§‘ (ëª¨ë‹¬ ì²˜ë¦¬)
                        product_data = self.scrape_nutrition_modal(menu_name, cat_name)
                        
                        # ì›ì¬ë£Œëª…(ì„¤ëª…) ì¶”ê°€ ìˆ˜ì§‘
                        try:
                            desc = self.driver.find_element(By.CSS_SELECTOR, ".description span").text.strip()
                            product_data['ingredients_raw'] = desc
                        except:
                            pass
                            
                        all_products.append(product_data)
                        
                        # ë¦¬ìŠ¤íŠ¸ë¡œ ë³µê·€
                        self.driver.back()
                        self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, ".menu_list_wrap")))
                        time.sleep(1.5)
                        
                    except Exception as e:
                        print(f"\n   âŒ ë©”ë‰´ ì—ëŸ¬: {menu_name} - {e}")
                        try: self.driver.back(); time.sleep(2)
                        except: pass
                        continue

            except Exception as e:
                print(f"âŒ ê·¸ë£¹ ì²˜ë¦¬ ì—ëŸ¬: {e}")
                continue

        # CSV ì €ì¥
        if all_products:
            df = pd.DataFrame(all_products)
            os.makedirs(settings.DATA_RAW, exist_ok=True)
            filepath = os.path.join(settings.DATA_RAW, 'burgerking_products.csv')
            
            columns = [
                'store_name', 'menu_name', 'price', 'calories', 'carbs', 'sugars', 
                'protein', 'fat', 'saturated_fat', 'trans_fat', 'cholesterol', 
                'sodium', 'allergens_scraped', 'ingredients_raw', 'category'
            ]
            
            for col in columns:
                if col not in df.columns: df[col] = 0 if col not in ['store_name', 'menu_name', 'allergens_scraped', 'ingredients_raw', 'category'] else ''
                
            df.to_csv(filepath, index=False, columns=columns, encoding='utf-8-sig')
            print(f"\n\nğŸ‰ ë²„ê±°í‚¹ ì €ì¥ ì™„ë£Œ: {filepath} (ì´ {len(df)}ê°œ ë©”ë‰´)")
        else:
            print("\nâš ï¸ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    def close(self):
        self.driver.quit()

if __name__ == "__main__":
    crawler = BurgerKingCrawler(headless=False)
    try:
        crawler.run()
    except KeyboardInterrupt:
        print("\nì‚¬ìš©ì ì¤‘ë‹¨")
    finally:
        crawler.close()